# üöÄ LOCAL AI SETUP - QUICK START

## ‚úÖ What Changed

I've added **local AI model support** to your project! Now you can:
- Download AI models **directly to your project folder** (`./models/`)
- No system-wide Ollama installation needed
- Models are portable - move with your project
- Works 100% offline after download

## üìã Files Created

1. **local_model_manager.py** - Manages local AI models
2. **setup_local_ai.py** - Interactive setup wizard
3. **LOCAL_AI_GUIDE.md** - Complete documentation
4. **Updated code_analyzer.py** - Now uses local models first

## üéØ How to Use

### Option 1: Quick Auto-Setup

```bash
# 1. Install required package
pip install llama-cpp-python

# 2. Run setup wizard
python setup_local_ai.py
```

The setup wizard will:
- Show available models (TinyLlama, DeepSeek-Coder, Phi-2)
- Download your chosen model to `./models/` folder
- Test that it works
- You're done!

### Option 2: Manual Setup

```bash
# Install package
pip install llama-cpp-python

# Test the model manager
python local_model_manager.py
```

Then choose a model to download from the list.

## üì¶ Recommended Models

**For Getting Started:**
- **TinyLlama** (700MB) - Smallest, fastest, perfect for testing

**For Best Code Analysis:**
- **DeepSeek-Coder 1.3B** (800MB) - Specialized for code

**For Best Quality:**
- **Phi-2** (1.6GB) - Microsoft's model, most capable

## ‚ö° What Happens Now

Once you download a model:

1. **Flask auto-detects it** on startup
2. **Code Analysis uses it automatically**
3. **No Ollama needed** (but still works as fallback)
4. **Works offline** after download

## üîç How It Works

### When you run Flask:
```
üîç Checking for local AI models...
üì¶ Found downloaded model: tinyllama
‚úÖ Using local model: tinyllama
üöÄ Starting Flask server at http://127.0.0.1:5000
```

### Priority order:
1. ‚úÖ Local models in `./models/` folder (tries first)
2. ‚úÖ Ollama at localhost:11434 (fallback)
3. ‚úÖ Basic analysis without AI (final fallback)

## üìä Comparison: Local vs Ollama

| Feature | Local Models | Ollama |
|---------|--------------|--------|
| Installation | Project only | System-wide |
| Location | `./models/` folder | System folders |
| Portable | ‚úÖ Yes (with project) | ‚ùå No |
| Offline | ‚úÖ Works offline | ‚ùå Needs service running |
| Setup | Download once | Install + download |
| Speed | ‚ö° Same speed | ‚ö° Same speed |
| Quality | ‚ö° Same quality | ‚ö° Same quality |

## üß™ Quick Test

After downloading a model:

```bash
# Test the model manager
python local_model_manager.py

# You should see:
# ‚úÖ Model loaded: tinyllama
# üìù Response: [AI-generated code]
```

## üÜò Troubleshooting

### Installation Issues

If `pip install llama-cpp-python` fails:

**Option A: Use pre-built wheels**
```bash
pip install llama-cpp-python --prefer-binary
```

**Option B: Use conda (if you have it)**
```bash
conda install -c conda-forge llama-cpp-python
```

**Option C: Stick with Ollama**
If installation is too complex, just use Ollama instead:
1. Download from: https://ollama.com/download
2. Run: `ollama pull llama3.2`
3. Your app will use it automatically

### "Model download failing"
- Check internet connection
- Try smaller model first (TinyLlama)
- Script auto-retries and cleans up failed downloads

### "Not using local model"
1. Check `./models/` folder exists and has `.gguf` files
2. Restart Flask server
3. Look for "‚úÖ Using local model" in startup logs

## üéØ Next Steps

### To use local models:
```bash
# 1. Install package (one-time)
pip install llama-cpp-python

# 2. Run setup (downloads model)
python setup_local_ai.py

# 3. Restart Flask
python flask_app.py

# 4. Hard refresh browser (Ctrl+Shift+R)

# 5. Try Code Analysis tab!
```

### To use Ollama instead:
```bash
# 1. Download Ollama
https://ollama.com/download

# 2. Pull model
ollama pull llama3.2

# 3. Done! (runs automatically on Windows)
```

## üí° Pro Tips

1. **Start small**: Try TinyLlama first (700MB)
2. **Test before Flask**: Run `python local_model_manager.py` to test
3. **Check the logs**: Flask startup shows which model it's using
4. **GPU support**: For faster inference, see LOCAL_AI_GUIDE.md

## üìù Summary

**What you have now:**
- ‚úÖ `local_model_manager.py` - Downloads & manages models
- ‚úÖ `setup_local_ai.py` - Interactive setup wizard
- ‚úÖ `code_analyzer.py` - Uses local models automatically
- ‚úÖ `LOCAL_AI_GUIDE.md` - Complete documentation

**What to do:**
1. `pip install llama-cpp-python`
2. `python setup_local_ai.py`
3. Choose a model and download
4. Restart Flask
5. Enjoy AI features without Ollama!

**Still prefer Ollama?**
That's fine! Local models are optional. Ollama still works as before.

---

**Questions?** See **LOCAL_AI_GUIDE.md** for detailed documentation!
