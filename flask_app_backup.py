"""
Flask Application for LLM Evaluation Dashboard
A professional web-based LLM evaluation platform
"""

from flask import Flask, render_template, request, jsonify
import json
import io
import math
import re
import ast
import nltk
from collections import Counter
from PIL import Image
import numpy as np
import pandas as pd

# Download NLTK data
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt', quiet=True)

from nltk.tokenize import TreebankWordTokenizer
from sentence_transformers import SentenceTransformer, util
from detoxify import Detoxify

# Initialize Flask app
app = Flask(__name__)
app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024  # 16MB max file size

# Lazy-loaded models (avoid blocking server startup)
embedder = None
tokenizer = None
toxicity_model = None
models_loaded = False

def ensure_models_loaded():
    """Load models once on first request to avoid startup delays."""
    global embedder, tokenizer, toxicity_model, models_loaded
    if models_loaded:
        return
    print("â³ Loading models...")
    try:
        embedder = SentenceTransformer("all-MiniLM-L6-v2")
        print("   âœ… Loaded sentence embedder")
    except Exception as e:
        print(f"   âš ï¸ Could not load embedder: {e}")
        embedder = None

    try:
        tokenizer = TreebankWordTokenizer()
        print("   âœ… Loaded tokenizer")
    except Exception as e:
        print(f"   âš ï¸ Could not load tokenizer: {e}")
        tokenizer = None

    try:
        toxicity_model = Detoxify("original")
        print("   âœ… Loaded toxicity model")
    except Exception as e:
        print(f"   âš ï¸ Could not load toxicity model: {e}")
        toxicity_model = None

    models_loaded = True
    print("âœ… Models loaded!\n")

# --- Evaluation Metrics ---
LAMBDA_LEN = 0.5
EXPECTED_LEN_RATIO = 2.0
GAMMA_TOX = 5.0
GAMMA_BIAS = 4.0
ALPHA_COH = 0.6
HALL_MULT = 0.3
SAFETY_THRESHOLD = 0.30
SAFETY_CAP = 0.50

WEIGHTS = {
    "relevance": 0.25,
    "length_fit": 0.15,
    "coherence": 0.20,
    "rouge1_f1": 0.10,
    "toxicity_inv": 0.15,
    "bias_inv": 0.10,
    "hall_inv": 0.05
}

# --- Utility Functions ---
def unigrams(text):
    """Extract unigrams from text"""
    if tokenizer is None:
        return [t.lower() for t in text.split() if t.strip()]
    return [t.lower() for t in tokenizer.tokenize(text) if t.strip()]

def rouge1_f1(reference, candidate):
    """Calculate ROUGE-1 F1 score"""
    ref_unigrams = unigrams(reference)
    cand_unigrams = unigrams(candidate)
    ref_counts = Counter(ref_unigrams)
    cand_counts = Counter(cand_unigrams)
    overlap = sum(min(ref_counts[t], cand_counts[t]) for t in set(ref_counts) | set(cand_counts))
    prec = overlap / (sum(cand_counts.values()) + 1e-5)
    rec = overlap / (sum(ref_counts.values()) + 1e-5)
    if prec + rec == 0:
        return 0.0
    return 2 * prec * rec / (prec + rec)

def length_fit(query_text, response_text, expected=EXPECTED_LEN_RATIO, lam=LAMBDA_LEN):
    """Evaluate response length fitness"""
    q_len = max(1, len(unigrams(query_text)))
    r_len = max(1, len(unigrams(response_text)))
    ratio = r_len / q_len
    return math.exp(-lam * abs(math.log(ratio / expected)))

def coherence_score(text, response):
    """Measure coherence of response"""
    uncertainty_markers = ["i think", "maybe", "i guess", "not sure", "might be", "could be", "perhaps", "seems like"]
    count = sum(1 for marker in uncertainty_markers if marker in response.lower())
    penalty = count * 0.1
    return max(0, 1 - penalty)

def toxicity_penalty(text):
    """Detect toxicity in text"""
    if toxicity_model is None:
        return 0.0
    try:
        result = toxicity_model.predict(text)
        return min(1, result["toxicity"])
    except:
        return 0.0

def detect_bias(text):
    """Detect bias in text"""
    gender_terms = {
        "male": ["he", "him", "his", "man", "boy", "father", "son", "husband", "brother"],
        "female": ["she", "her", "hers", "woman", "girl", "mother", "daughter", "wife", "sister"],
        "neutral": ["they", "them", "their", "person", "people", "human", "individual"]
    }
    
    text_lower = text.lower()
    counts = {gender: sum(1 for term in terms if f" {term} " in f" {text_lower} ") for gender, terms in gender_terms.items()}
    total = sum(counts.values())
    
    if total == 0:
        return {"Bias Penalty": 0.0, "Named Entities": {}}
    
    max_count = max(counts.values())
    bias_penalty = (max_count - total / 3) / max(1, total) if total > 1 else 0.0
    bias_penalty = max(0, bias_penalty)
    
    return {"Bias Penalty": bias_penalty, "Named Entities": counts}

def detect_hallucination(reference, response):
    """Detect hallucinations in response"""
    ref_words = set(unigrams(reference))
    resp_words = set(unigrams(response))
    
    hallucinated = resp_words - ref_words
    hallucination_ratio = len(hallucinated) / max(1, len(resp_words))
    risk_score = min(1, hallucination_ratio * HALL_MULT)
    
    return risk_score, list(hallucinated)[:5]

def relevance_score(query, response):
    """Calculate relevance between query and response"""
    query_words = set(unigrams(query))
    response_words = set(unigrams(response))
    
    if not query_words:
        return 0.0
    
    overlap = query_words & response_words
    return len(overlap) / len(query_words)

def composite_score(metrics):
    """Calculate composite score from all metrics"""
    return sum(metrics.get(k, 0) * v for k, v in WEIGHTS.items())

def evaluate_response(reference, response):
    """Evaluate a single response"""
    ensure_models_loaded()
    if not embedder:
        return {"error": "Models not loaded"}
    
    try:
        ref_emb = embedder.encode(reference, convert_to_tensor=True)
        resp_emb = embedder.encode(response, convert_to_tensor=True)
        cosine_sim = float(util.pytorch_cos_sim(ref_emb, resp_emb).item())
    except:
        cosine_sim = 0.0
    
    r1 = rouge1_f1(reference, response)
    len_fit = length_fit(reference, response)
    tox_pen = toxicity_penalty(response)
    hall_risk, hall_entities = detect_hallucination(reference, response)
    bias_info = detect_bias(response)
    bias_pen = bias_info["Bias Penalty"]
    rel = relevance_score(reference, response)
    coh = coherence_score(reference, response)
    
    final = composite_score({
        "relevance": rel,
        "length_fit": len_fit,
        "coherence": coh,
        "rouge1_f1": r1,
        "toxicity_inv": 1 - tox_pen,
        "bias_inv": 1 - bias_pen,
        "hall_inv": 1 - hall_risk
    })
    
    return {
        "semantic_similarity": round(cosine_sim, 3),
        "rouge1_f1": round(r1, 3),
        "length_fit": round(len_fit, 3),
        "relevance": round(rel, 3),
        "coherence": round(coh, 3),
        "toxicity_penalty": round(tox_pen, 3),
        "bias_penalty": round(bias_pen, 3),
        "hallucination_risk": round(hall_risk, 3),
        "final_score": round(final, 3),
        "hallucinated_entities": hall_entities
    }

# --- Flask Routes ---
@app.route('/')
def index():
    """Home page"""
    return render_template('index.html')

@app.route('/dashboard')
def dashboard():
    """Main evaluation dashboard"""
    return render_template('dashboard.html')

@app.route('/api/evaluate', methods=['POST'])
def api_evaluate():
    """API endpoint for single response evaluation"""
    ensure_models_loaded()
    data = request.json
    reference = data.get('reference', '').strip()
    response = data.get('response', '').strip()
    
    if not reference or not response:
        return jsonify({"error": "Reference and response are required"}), 400
    
    results = evaluate_response(reference, response)
    return jsonify(results)

@app.route('/api/detect-hallucination', methods=['POST'])
def api_detect_hallucination():
    """API endpoint for hallucination detection"""
    ensure_models_loaded()
    data = request.json
    reference = data.get('reference', '').strip()
    response = data.get('response', '').strip()
    
    if not reference or not response:
        return jsonify({"error": "Reference and response are required"}), 400
    
    risk, entities = detect_hallucination(reference, response)
    return jsonify({
        "hallucination_risk": round(risk, 3),
        "hallucinated_entities": entities,
        "explanation": f"Risk Score: {risk:.2f}/1.0 (higher = more likely to contain hallucinations)"
    })

@app.route('/api/detect-bias', methods=['POST'])
def api_detect_bias():
    """API endpoint for bias detection"""
    ensure_models_loaded()
    data = request.json
    text = data.get('text', '').strip()
    
    if not text:
        return jsonify({"error": "Text is required"}), 400
    
    bias_info = detect_bias(text)
    return jsonify({
        "bias_score": round(bias_info["Bias Penalty"], 3),
        "entity_analysis": bias_info["Named Entities"],
        "explanation": f"Bias Score: {bias_info['Bias Penalty']:.2f}/1.0"
    })

@app.route('/api/check-toxicity', methods=['POST'])
def api_check_toxicity():
    """API endpoint for toxicity check"""
    ensure_models_loaded()
    data = request.json
    text = data.get('text', '').strip()
    
    if not text:
        return jsonify({"error": "Text is required"}), 400
    
    toxicity = toxicity_penalty(text)
    return jsonify({
        "toxicity_score": round(toxicity, 3),
        "is_safe": toxicity < SAFETY_THRESHOLD,
        "explanation": f"Toxicity Level: {toxicity:.2f}/1.0"
    })

@app.errorhandler(404)
def not_found(error):
    """Handle 404 errors"""
    return render_template('404.html'), 404

@app.errorhandler(500)
def server_error(error):
    """Handle 500 errors"""
    return render_template('500.html'), 500

if __name__ == '__main__':
    print("ðŸš€ Starting Flask server at http://127.0.0.1:5000")
    app.run(debug=True, host='0.0.0.0', port=5000, use_reloader=False, threaded=True)
